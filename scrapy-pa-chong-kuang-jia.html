<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Scrapy爬虫框架, Charles Blog">
    <meta name="baidu-site-verification" content="fmlEuI34ir">
    <meta name="google-site-verification" content="yCy2azpds5XSuGZvis6OuA-XIGF5GuGpYRAaGfD6o48">
    <meta name="360-site-verification" content="b7c11a830ef90fd1464ad6206bb7b6e7">
    <meta name="description" content="简介Scrapy爬虫框架是一个爬取效率高、相关扩展组件多，可以让程序员将精力全部 投入到抓取规则以及数据处理上的一款优秀框架。
Scrapy不仅可以应用到网络爬虫中，还可以用于数据挖掘、数据监测以及自动化测试等。Scrapy是基于Twist">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>Scrapy爬虫框架 | Charles Blog</title>
    <link rel="icon" type="image/jpeg" href="/favicon.jpg">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>

    <script>
    var _hmt = _hmt || [];
    (function() {
    var hm = document.createElement("script");
    hm.src = "https://hm.baidu.com/hm.js?4d1d73af45a62734730491a6b6c41da4";
    var s = document.getElementsByTagName("script")[0]; 
    s.parentNode.insertBefore(hm, s);
    })();
   </script>


    <script>(function (i, s, o, g, r, a, m) {
        i['DaoVoiceObject'] = r;
        i[r] = i[r] ||
          function () {
            (i[r].q = i[r].q || []).push(arguments);
          };
        i[r].l = 1 * new Date();
        a = s.createElement(o);
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        a.charset = 'utf-8';
        m.parentNode.insertBefore(a, m);
      })(window, document, 'script', ('https:' === document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/xxx.js", 'daovoice');
      daovoice('init', {
        app_id: "xxx",
      });
      daovoice('update');
    </script>
  
  


    
        <script>
            (function(){
                var bp = document.createElement('script');
                var curProtocol = window.location.protocol.split(':')[0];
                if (curProtocol === 'https') {
                    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
                }
                else {
                    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
                }
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(bp, s);
            })();
        </script>
    

    <script>
        (function(){
        var src = "https://jspassport.ssl.qhimg.com/11.0.1.js?d182b3f28525f2db83acfaaf6e696dba";
        document.write('<script src="' + src + '" id="sozz"><\/script>');
        })();
    </script>

    <meta name="baidu-site-verification" content>




<style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: #FFF;
        text-align: center;
        /* loaderҳ����ʧ���ý����ķ�ʽ*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: #49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: #2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo���ֶ��� */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ʹ�ý����ķ�������loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },1000); 
        },1000);//ǿ����ʾloading page 1s  
    };
    loaded();
})()
 </script><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"><style type="text/css" lang="css">
    #loading-container{
        position: fixed;
        top: 0;
        left: 0;
        min-height: 100vh;
        width: 100vw;
        z-index: 9999;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        background: #FFF;
        text-align: center;
        /* loaderҳ����ʧ���ý����ķ�ʽ*/
        -webkit-transition: opacity 1s ease;
        -moz-transition: opacity 1s ease;
        -o-transition: opacity 1s ease;
        transition: opacity 1s ease;
    }
    .loading-image{
        width: 120px;
        height: 50px;
        transform: translate(-50%);
    }

    .loading-image div:nth-child(2) {
        -webkit-animation: pacman-balls 1s linear 0s infinite;
        animation: pacman-balls 1s linear 0s infinite
    }

    .loading-image div:nth-child(3) {
        -webkit-animation: pacman-balls 1s linear .33s infinite;
        animation: pacman-balls 1s linear .33s infinite
    }

    .loading-image div:nth-child(4) {
        -webkit-animation: pacman-balls 1s linear .66s infinite;
        animation: pacman-balls 1s linear .66s infinite
    }

    .loading-image div:nth-child(5) {
        -webkit-animation: pacman-balls 1s linear .99s infinite;
        animation: pacman-balls 1s linear .99s infinite
    }

   .loading-image div:first-of-type {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_up .5s 0s infinite;
        animation: rotate_pacman_half_up .5s 0s infinite;
    }
    .loading-image div:nth-child(2) {
        width: 0;
        height: 0;
        border: 25px solid #49b1f5;
        border-right-color: transparent;
        border-radius: 25px;
        -webkit-animation: rotate_pacman_half_down .5s 0s infinite;
        animation: rotate_pacman_half_down .5s 0s infinite;
        margin-top: -50px;
    }
    @-webkit-keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @keyframes rotate_pacman_half_up {0% {transform: rotate(270deg)}50% {transform: rotate(1turn)}to {transform: rotate(270deg)}}

    @-webkit-keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @keyframes rotate_pacman_half_down {0% {transform: rotate(90deg)}50% {transform: rotate(0deg)}to {transform: rotate(90deg)}}

    @-webkit-keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}

    @keyframes pacman-balls {75% {opacity: .7}to {transform: translate(-100px, -6.25px)}}


    .loading-image div:nth-child(3),
    .loading-image div:nth-child(4),
    .loading-image div:nth-child(5),
    .loading-image div:nth-child(6){
        background-color: #49b1f5;
        width: 15px;
        height: 15px;
        border-radius: 100%;
        margin: 2px;
        width: 10px;
        height: 10px;
        position: absolute;
        transform: translateY(-6.25px);
        top: 25px;
        left: 100px;
    }
    .loading-text{
        margin-bottom: 20vh;
        text-align: center;
        color: #2c3e50;
        font-size: 2rem;
        box-sizing: border-box;
        padding: 0 10px;
        text-shadow: 0 2px 10px rgba(0,0,0,0.2);
    }
    @media only screen and (max-width: 500px) {
         .loading-text{
            font-size: 1.5rem;
         }
    }
    .fadeout {
        opacity: 0;
        filter: alpha(opacity=0);
    }
    /* logo���ֶ��� */
    @-webkit-keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);transform:translate3d(0,-100%,0)}100%{opacity:1;-webkit-transform:none;transform:none}}
    @keyframes fadeInDown{0%{opacity:0;-webkit-transform:translate3d(0,-100%,0);}}
 </style>
 <script>
(function () {
    const loaded = function(){
       setTimeout(function(){
            const loader = document.getElementById("loading-container");
            loader.className="fadeout" ;//ʹ�ý����ķ�������loading page
            // document.getElementById("body-wrap").style.display="flex";
            setTimeout(function(){
                loader.style.display="none";
            },1000); 
        },1000);//ǿ����ʾloading page 1s  
    };
    loaded();
})()
 </script></head>


 <div id="loading-container">
     <p class="loading-text"></p> 
     <div class="loading-image">
         <div></div>
         <div></div>
         <div></div>
         <div></div> 
         <div></div>
     </div>
 </div><body>

    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.jpg" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Charles Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/contact" class="waves-effect waves-light">
            
            <span>留言板</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="搜索"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.jpg" class="logo-img circle responsive-img">
        
        <div class="logo-name">Charles Blog</div>
        <div class="logo-desc">
            
            C
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                友情链接
            </a>
        </li>
        
        <li>
            <a href="/contact" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-link"></i>
                
                留言板
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/charles-li-github" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>

        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/charles-li-github" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('请输入访问本文章的密码')).toString(CryptoJS.enc.Hex)) {
                alert('密码错误，将返回主页！');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/17.jpg')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        Scrapy爬虫框架
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                        <a href="/tags/python/" target="_blank">
                            <span class="chip bg-color">python</span>
                        </a>
                        
                        <a href="/tags/Scrapy/" target="_blank">
                            <span class="chip bg-color">Scrapy</span>
                        </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                        <a href="/categories/网络爬虫/" class="post-category" target="_blank">
                            网络爬虫
                        </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-09-26
                </div>

                <div class="post-author info-break-policy">
                    <i class="fa fa-user-o fa-fw"></i>作者:&nbsp;&nbsp;
                    
                    CharlesLi
                    
                </div>

                
                
                <div class="info-break-policy">
                    <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                    8.8k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                    34 分
                </div>
                
                

                
                <div id="busuanzi_container_page_pv" class="info-break-policy">
                    <i class="fa fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                    <span id="busuanzi_value_page_pv"></span>
                </div>
                
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Scrapy爬虫框架是一个爬取效率高、相关扩展组件多，可以让程序员将精力全部 投入到抓取规则以及数据处理上的一款优秀框架。</p>
<p>Scrapy不仅可以应用到网络爬虫中，还可以用于数据挖掘、数据监测以及自动化测试等。Scrapy是基于Twisted的<code>异步处理框架</code>，结构清晰、可扩展性强，可以灵活地完成各种需求。Scrapy框架的整体架构如下：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926111646219.png" alt="image-20210926111646219"></p>
<p>在SCrapy的工作流程中主要包含以下几个部分：</p>
<ul>
<li>Scrapy Engine（框架的引擎）：用于处理整个系统的数据流，触发各种事件，是整个框架的核心。</li>
<li>Scheduler（调度器）：用于接收引擎发过来的请求，添加至队列中，在引擎再次请求时将请求返回给引擎。可以理解为从URL队列中取出一个请求地址，同时去除重复的请求地址。</li>
<li>Downloader（下载器）：用于从网络下载Web资源。</li>
<li>Spiders（网络爬虫）：从指定网页中爬取需要的信息。</li>
<li>Item Pipeline（项目管道）：用于处理爬取后的数据，例如数据的清洗、验证以及保存。</li>
<li>Downloader Middlewares（下载器中间件）：位于Scrapy引擎和下载器之间，主要用于处理引擎与下载器之间的网络请求与响应。</li>
<li>Spider Middlewares（爬虫中间件）：位于爬虫与引擎之间，主要用于处理爬虫的响应输入和请求输出。</li>
<li>Scheduler Middlewares（调度中间件）：位于引擎和调度之间，主要用于处理从引擎发送到调度的请求和响应。</li>
</ul>
<h2 id="搭建Scrapy爬虫框架"><a href="#搭建Scrapy爬虫框架" class="headerlink" title="搭建Scrapy爬虫框架"></a>搭建Scrapy爬虫框架</h2><h3 id="使用Anaconda安装Scrapy"><a href="#使用Anaconda安装Scrapy" class="headerlink" title="使用Anaconda安装Scrapy"></a>使用Anaconda安装Scrapy</h3><p>如果使用Anaconda，则可以在Anaconda Prompt（Anaconda）窗口中输入“<code>conda install scrapy</code>”命令进行Scrapy框架的安装。不过安装过程中可能出现下述404错误</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926113301905.png" alt="image-20210926113301905"></p>
<p>如果出现404错误，首先需要通过“<code>conda config --show-sources</code>”命令查看是否存在镜像地址</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926113400792.png" alt="image-20210926113400792"></p>
<p>经过查询发现存在镜像地址时，可以先通过“<code>conda config --remove-key channels</code>”命令清空所有镜像地址，然后再次通过“<code>conda config --show-sources</code>”命令查看</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926113550832.png" alt="image-20210926113550832"></p>
<p>镜像地址被清空后，再次输入“<code>conda install scrapy</code>”命令安装Scrapy爬虫框架</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926113631948.png" alt="image-20210926113631948"></p>
<p>在底部命令行输入y，确认继续安装Scrapy爬虫框架</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926113707307.png" alt="image-20210926113707307"></p>
<h3 id="Windows系统下安装Scrapy"><a href="#Windows系统下安装Scrapy" class="headerlink" title="Windows系统下安装Scrapy"></a>Windows系统下安装Scrapy</h3><p>如果不使用Anaconda，在Windows系统下安装，Scrapy至少需要依赖的库由twisted、lxml、pyOpenSSL以及pywin32。</p>
<h4 id="安装twisted模块"><a href="#安装twisted模块" class="headerlink" title="安装twisted模块"></a>安装twisted模块</h4><ul>
<li>打开Python扩展包的非官方Windows二进制文件网站（<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/），然后按下快捷键“Ctrl+F”搜索“twisted”模块。" target="_blank" rel="noopener">https://www.lfd.uci.edu/~gohlke/pythonlibs/），然后按下快捷键“Ctrl+F”搜索“twisted”模块。</a></li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926114453190.png" alt="image-20210926114453190"></p>
<ul>
<li>“cp38”表示Python3.8版本；“win32”与“win_amd64”分别表示Windows的32位与64位系统</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926114725101.png" alt="image-20210926114725101"></p>
<p>进入上述文件的下载目录，执行“<code>pip install Twisted-20.3.0-cp38-cp38-win_amd64.whl</code>”安装twisted模块</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926114851418.png" alt="image-20210926114851418"></p>
<h4 id="安装Scrapy"><a href="#安装Scrapy" class="headerlink" title="安装Scrapy"></a>安装Scrapy</h4><p>打开命令提示符，然后输入“<code>pip install Scrapy</code>”命令，安装Scrapy</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926114959597.png" alt="image-20210926114959597"></p>
<blockquote>
<p>Scrapy框架在安装的过程中，同时会将lxml与pyOpenSSL模块也安装在python环境当中</p>
</blockquote>
<h4 id="安装pywin32"><a href="#安装pywin32" class="headerlink" title="安装pywin32"></a>安装pywin32</h4><p>打开命令提示符，然后输入“<code>pip install pywin32</code>”命令，安装pywin32模块。安装完成后在python命令行中输入“<code>import pywin32_system32</code>”，如果没有提示错误信息，则表示安装成功。</p>
<h2 id="Scrapy的基本应用"><a href="#Scrapy的基本应用" class="headerlink" title="Scrapy的基本应用"></a>Scrapy的基本应用</h2><h3 id="创建Scrapy项目"><a href="#创建Scrapy项目" class="headerlink" title="创建Scrapy项目"></a>创建Scrapy项目</h3><p>在任意路径下创建一个保存项目的文件夹，然后在此路径文件夹中运行命令窗口并输入“<code>scrapy startproject scrapyDemo</code>”，即可创建一个名称为“<code>scrapyDemo</code>”的项目</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926134540618.png" alt="image-20210926134540618"></p>
<p>可使用PyCharm打开scrapyDemo项目，项目目录结构如下：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926134716830.png" alt="image-20210926134716830"></p>
<p><strong>目录结构文件说明：</strong></p>
<ul>
<li>spiders（文件夹）：用于创建爬虫文件，编写爬虫规则</li>
<li>__init__.py文件：初始化文件</li>
<li>items.py文件：用于数据的定义，可以寄存处理后的数据</li>
<li>middlewares.py文件：定义爬取时的中间件，其中包括SpiderMiddleware（爬虫中间件）、DownloaderMiddleware（下载中间件）</li>
<li>pipelines.py文件：用于实现清洗数据、验证数据、保存数据</li>
<li>settings.py文件：整个框架的配置文件，主要包含配置爬虫信息，请求头、中间件等</li>
<li>scrapy.cfg文件：项目部署文件，其中定义了项目的配置文件路径等相关信息</li>
</ul>
<h3 id="创建爬虫"><a href="#创建爬虫" class="headerlink" title="创建爬虫"></a>创建爬虫</h3><p>在创建爬虫时，首先需要创建一个爬虫模块文件，该文件需要放置在spiders文件夹当中。爬虫模块时用于从一个网站或多个网站中爬取数据的类，他需要继承scrapy.Spider类，scrapy.Spider类中提供了start_requests()方法实现初始化网络请求，然后通过parse()方法解析返回的结果。</p>
<p>scrapy.Spider类中的常用属性与方法含义如下：</p>
<ul>
<li>name：用于定义一个爬虫名称的字符串。Scrapy通过这个爬虫名称进行爬虫的查找，所以这个名称必须是唯一的，不过我们可以生成多个相同的爬虫实例。如爬取单个网站一般会用这个网站的名称作为爬虫的名称</li>
<li>allowed_domains：包含了爬虫允许爬取的域名列表，当OffsiteMiddleware启用时，域名不在列表中的URL不会被爬取</li>
<li>start_urls：URL的初始列表，如果没有指定特定的URL，爬虫将从该列表中进行爬取</li>
<li>custom_settings：这是一个专属于当前爬虫的配置，是一个字典类型的数据，设置该属性会覆盖整个项目的全局，所以在设置该属性时必须在实例化前更新，必须定义为类变量</li>
<li>settings：这是一个settings对象，通过它，我们可以获取项目的全局设置变量</li>
<li>logger：使用Spider创建的Python日志器</li>
<li>start_requests()：该方法用于生成网络请求，它必须返回一个可迭代对象。该方法默认使用start_urls中的URL来生成request，而request的请求方式为GET，如果我们想通过POST方式请求网页时，可以使用FormRequest()重写该方法</li>
<li>parse()：如果response没有指定回调函数时，该方法是Scrapy处理response的默认方法。该方法负责处理response并返回处理的数据和下一步请求，然后返回一个包含request或Item的可迭代对象</li>
<li>closed()：当爬虫关闭时，该函数会被调用。该方法用于代替监听工作，可以定义释放资源或是收尾操作</li>
</ul>
<p><strong>以爬取下图网页为例，实现爬取网页后将王爷的代码以HTML文件形式保存至项目文件夹中</strong></p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926140902360.png" alt="image-20210926140902360"></p>
<p>在spiders文件夹当中创建一个名为“crawl.py”的爬虫文件，然后在该文件中：</p>
<ul>
<li>首先创建QuotesSpider类，该类需要继承自scrapy.Spider类</li>
<li>然后<code>重写start_requests()方法</code>实现网络的请求工作</li>
<li>接着<code>重写parse()方法</code>实现向文件中写入获取的html代码</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy  <span class="token comment" spellcheck="true"># 导入框架</span>
<span class="token keyword">import</span> twisted

<span class="token keyword">class</span> <span class="token class-name">QuotesSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"quotes"</span>  <span class="token comment" spellcheck="true"># 定义爬虫名称</span>

    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 设置爬取目标的地址</span>
        urls <span class="token operator">=</span> <span class="token punctuation">[</span>
            <span class="token string">'http://quotes.toscrape.com/page/1/'</span><span class="token punctuation">,</span>
            <span class="token string">'http://quotes.toscrape.com/page/2/'</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span>
        <span class="token comment" spellcheck="true"># 获取所有地址，有几个地址发送几次请求</span>
        <span class="token keyword">for</span> url <span class="token keyword">in</span> urls<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># 发送网络请求</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 获取页数</span>
        page <span class="token operator">=</span> response<span class="token punctuation">.</span>url<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"/"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span>
        <span class="token comment" spellcheck="true"># 根据页数设置文件名称</span>
        filename <span class="token operator">=</span> <span class="token string">'quotes-%s.html'</span> <span class="token operator">%</span> page
        <span class="token comment" spellcheck="true"># 写入文件的模式打开文件，如果没有该文件将创建该文件</span>
        <span class="token keyword">with</span> open<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> <span class="token string">'wb'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># 像文件中写入获取的html代码</span>
            f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>response<span class="token punctuation">.</span>body<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 输出保存文件的名称</span>
        self<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token string">'Saved file %s'</span> <span class="token operator">%</span> filename<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>在运行Scrapy所创建的爬虫项目时，需要在命令窗口中输入“<code>scrapy crawl quotes</code>”，其中“<code>quotes</code>”是自己定义的爬虫名称。PyCharm则需要在底部的Terminal窗口中输入上述命令</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926141353432.png" alt="image-20210926141353432"></p>
<p>如果我们要实现一个POST请求，可以使用FormRequest()函数来实现，示例代码如下</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy  <span class="token comment" spellcheck="true"># 导入框架</span>
<span class="token keyword">import</span> json

<span class="token keyword">class</span> <span class="token class-name">QuotesSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">"quotes"</span>  <span class="token comment" spellcheck="true"># 定义爬虫名称</span>
    <span class="token comment" spellcheck="true"># 字典类型的表单参数</span>
    data <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'1'</span><span class="token punctuation">:</span><span class="token string">'能力是有限的，而努力是无限的。'</span><span class="token punctuation">,</span>
            <span class="token string">'2'</span><span class="token punctuation">:</span><span class="token string">'星光不问赶路人，时光不负有心人。'</span><span class="token punctuation">}</span>
    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>scrapy<span class="token punctuation">.</span>FormRequest<span class="token punctuation">(</span><span class="token string">'http://httpbin.org/post'</span><span class="token punctuation">,</span>formdata<span class="token operator">=</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># 响应信息</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        response_dict <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 将响应数据转换为字典类型</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response_dict<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印转换后的响应数据</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>除了在命令窗口中输入“<code>scrapy crawl quotes</code>”启动爬虫程序以外，Scrapy还提供了可以在程序中启动爬虫的API，也就是CrawlerProcess类。</p>
<ul>
<li>首先需要在CrawlerProcess类初始化时传入项目的settings信息，</li>
<li>然后在crawl()方法中传入爬虫的名称</li>
<li>最后通过start()方法启动爬虫</li>
</ul>
<p>代码如下：</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入CrawlerProcess类</span>
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>crawler <span class="token keyword">import</span> CrawlerProcess
<span class="token comment" spellcheck="true"># 导入获取项目设置信息</span>
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>project <span class="token keyword">import</span> get_project_settings


<span class="token comment" spellcheck="true"># 程序入口</span>
<span class="token keyword">if</span> __name__<span class="token operator">==</span><span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 创建CrawlerProcess类对象并传入项目设置信息参数</span>
    process <span class="token operator">=</span> CrawlerProcess<span class="token punctuation">(</span>get_project_settings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 设置需要启动的爬虫名称</span>
    process<span class="token punctuation">.</span>crawl<span class="token punctuation">(</span><span class="token string">'quotes'</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 启动爬虫</span>
    process<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>如果在运行Scrapy所创建的爬虫项目时，出现SyntaxError:invalid syntax的错误信息，如下图所示，说明Python3.7这个版本将“async”识别成了关键字，解决此类问题方法，首先需要打开Python37\Lib\site-packages\twisted\conch\manhole.py文件，然后将该文件中的所有“async”关键字修改成与关键字无关的标识符，如“async_”</p>
</blockquote>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926143907302.png" alt="image-20210926143907302"></p>
<h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><p>Scrapy爬虫框架可以通过特定的CSS或者XPath表达式来选择HTML文件中的某一处，而且提取出相应的数据。CSS（Cascading Style Sheet，即层叠样式表），用于控制HTML页面的布局、字体、颜色、背景以及其他效果。XPath是一门可以在XML文档中根据元素和属性查找信息的语言</p>
<h4 id="CSS提取数据"><a href="#CSS提取数据" class="headerlink" title="CSS提取数据"></a>CSS提取数据</h4><p>使用CSS提取HTML文件中的某一处数据时，可以指定HTML文件中的标签名称。例如，获取上个网页的title标签数据</p>
<pre class="line-numbers language-python"><code class="language-python">response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'title'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>获取结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926144330750.png" alt="image-20210926144330750"></p>
<blockquote>
<p>使用CSS提取数据时返回的内容为CSS表达式所对应节点的list列表，所以在提取标签中的数据时，可以使用以下代码</p>
</blockquote>
<pre class="line-numbers language-python"><code class="language-python">response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'title::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>或者是</p>
<pre class="line-numbers language-python"><code class="language-python">response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'title::text'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h4 id="XPath提取器"><a href="#XPath提取器" class="headerlink" title="XPath提取器"></a>XPath提取器</h4><p>使用XPath表达式提取HTML文件中的某一处数据时，需要根据XPath表达式的语法规定来获取指定的数据信息</p>
<pre class="line-numbers language-python"><code class="language-python">response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//title/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><strong>使用XPath表达式获取多条信息</strong></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 响应信息</span>
<span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 获取所有信息</span>
    <span class="token keyword">for</span> quote <span class="token keyword">in</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='quote']"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 获取名人名言文字信息</span>
        text <span class="token operator">=</span> quote<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='text']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 获取作者</span>
        author <span class="token operator">=</span> quote<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='author']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 获取标签</span>
        tags <span class="token operator">=</span> quote<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='tag']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 以字典形式输出信息</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>dict<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> author<span class="token operator">=</span>author<span class="token punctuation">,</span> tags<span class="token operator">=</span>tags<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>程序执行部分结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926144851384.png" alt="image-20210926144851384"></p>
<blockquote>
<p>Scrapy的选择对象中还提供了<code>.re()</code>方法，这是一种可以使用正则表达式提取数据的方法，可以直接通过<code>response.xpath.re()</code>方式进行调用，然后再re()方法中填入对应的正则表达式即可</p>
</blockquote>
<h4 id="翻页提取数据"><a href="#翻页提取数据" class="headerlink" title="翻页提取数据"></a>翻页提取数据</h4><p>如果需要<code>获取整个网页的所有信息</code>就需要使用到<code>翻页功能</code></p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 响应信息</span>
<span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># div.quote</span>
    <span class="token comment" spellcheck="true"># 获取所有信息</span>
    <span class="token keyword">for</span> quote <span class="token keyword">in</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='quote']"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 获取作者</span>
        author <span class="token operator">=</span> quote<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='author']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>author<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输出作者名称</span>

    <span class="token comment" spellcheck="true"># 实现翻页</span>
    <span class="token keyword">for</span> href <span class="token keyword">in</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'li.next a::attr(href)'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">yield</span> response<span class="token punctuation">.</span>follow<span class="token punctuation">(</span>href<span class="token punctuation">,</span> self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h4 id="创建Items"><a href="#创建Items" class="headerlink" title="创建Items"></a>创建Items</h4><p>爬取网页数据的过程就是从非结构性的数据源中提取结构性数据。例如，在QuotesSpider类的parse()方法中已经获取到了text、author以及tags信息，如果需要将这些数据包装成结构化数据，那么就需要使用Scrapy所提供的<code>Item类</code>来满足这样的需求。Item对象是一个简单的容器，用于保存爬取到的数据信息，它提供了一个类似于字典的API，用于声明其可用字段的便捷语法，Item使用简单的类定义语法和Field对象来声明。在创建scrapyDemo项目时，项目的目录结构中就已经自动创建了一个items.py文件，用来定义存储数据信息的Item类，它需要继承scrapy.Item。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy


<span class="token keyword">class</span> <span class="token class-name">ScrapydemoItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># define the fields for your item here like:</span>
    <span class="token comment" spellcheck="true"># 定义获取名人名言文字信息</span>
    text <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 定义获取的作者</span>
    author <span class="token operator">=</span>scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 定义获取的标签</span>
    tags <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>Item创建完成后，回到自己编写的爬虫代码中，在parse()方法中创建Item对象，然后返回item信息，代码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 响应信息</span>
<span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 获取所有信息</span>
    <span class="token keyword">for</span> quote <span class="token keyword">in</span> response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='quote']"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 获取名人名言文字信息</span>
        text <span class="token operator">=</span> quote<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='text']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 获取作者</span>
        author <span class="token operator">=</span> quote<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='author']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 获取标签</span>
        tags <span class="token operator">=</span> quote<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='tag']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># 创建Item对象</span>
        item <span class="token operator">=</span> ScrapydemoItem<span class="token punctuation">(</span>text<span class="token operator">=</span>text<span class="token punctuation">,</span> author<span class="token operator">=</span>author<span class="token punctuation">,</span> tags<span class="token operator">=</span>tags<span class="token punctuation">)</span>
        <span class="token keyword">yield</span> item  <span class="token comment" spellcheck="true"># 输出信息</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>程序执行部分结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926151250432.png" alt="image-20210926151250432"></p>
<h3 id="将爬取的数据保存为多种格式的文件"><a href="#将爬取的数据保存为多种格式的文件" class="headerlink" title="将爬取的数据保存为多种格式的文件"></a>将爬取的数据保存为多种格式的文件</h3><p>在确保已经创建了Item类以后，便可以轻松地将爬取到的数据保存成多种格式的文件，如JSON、CSV、XML等</p>
<p>例如，我们需要将每个Item信息写成1行JSON时，需要将数据写成后缀名为<code>.jl</code>或者<code>.jsonlines</code>的文件，可以在命令窗口中输入下面的命令：</p>
<pre><code>scrapy crawl quotes -o test.jl</code></pre><p>或</p>
<pre><code>scrapy crawl quotes -0 test.jsonlines</code></pre><blockquote>
<p>上述命令中：</p>
<ul>
<li>quotes为启动爬虫的名称</li>
<li>test表示保存后的文件名称</li>
<li><code>.jl</code>或<code>.jsonlines</code>表示保存文件的后缀名称</li>
</ul>
</blockquote>
<p>如果需要将数据保存成<code>.josn</code>、<code>.csv</code>、<code>.xml</code>、<code>.pickle</code>、<code>.marshal</code>文件，则可参考下列命令代码</p>
<pre><code>scrapy crawl quotes -o test.json
scrapy crawl quotes -o test.csv
scrapy crawl quotes -o test.xml
scrapy crawl quotes -o test.pickle
scrapy crawl quotes -o test.marshal</code></pre><p>如果我们不想通过命令行的方式保存各种格式的文件，可以使用Scrapy所提供的cmdline子模块，该模块中提供了execute()方法，该方法中的参数为列表参数，所以我们将命令行代码拆分成列表即可。示例代码如下：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> scrapy <span class="token keyword">import</span> cmdline
cmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'scrapy crawl quotes -o test.json'</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
cmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'scrapy crawl quotes -o test.csv'</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
cmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'scrapy crawl quotes -o test.xml'</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
cmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'scrapy crawl quotes -o test.pickle'</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
cmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">'scrapy crawl quotes -o test.marshal'</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<blockquote>
<p>上述示例代码不可同时执行，只能单跳命令执行</p>
</blockquote>
<h2 id="编写Item-Pipeline"><a href="#编写Item-Pipeline" class="headerlink" title="编写Item Pipeline"></a>编写Item Pipeline</h2><p>当爬取的数据已经被存放在Items以后，如果Spider（爬虫）解析完Response（响应结果），Items就会传递到Item Pipeline（项目管道）中，然后在Item Pipeline中创建用于处理数据的类，这个类就是项目管道组件，通过执行一连串处理即可实现数据的清洗、存储等工作。</p>
<h3 id="项目管道的核心写法"><a href="#项目管道的核心写法" class="headerlink" title="项目管道的核心写法"></a>项目管道的核心写法</h3><p>Item Pipeline（项目管道）的典型用途如下：</p>
<ul>
<li>清理HTML数据</li>
<li>验证抓取的数据（检查项目是否包含某些字段）</li>
<li>检查重复项（并将其删除）</li>
<li>将爬取的结果存储在数据库中</li>
</ul>
<p>在编写自定义Item Pipeline时，可以实现以下几个方法：</p>
<ul>
<li>process_item()：该方法是在自定义Item Pipeline时，所必须实现的方法。该方法中需要提供两个参数，参数含义如下：<ul>
<li>item参数为Item对象（被处理的Item）或字典</li>
<li>spider参数为Spider对象（爬取信息的爬虫）</li>
</ul>
</li>
</ul>
<blockquote>
<p>process_item()方法用于处理返回的Item对象，在处理时会先处理低优先级的Item对象，直到所有的方法调用完毕。如果返回Deferred或引发DropItem异常时，那么该Item对象将不再进行处理</p>
</blockquote>
<ul>
<li>open_spider()：该方法实在开启爬虫时被调用的，所以再这个方法中可以进行初始化操作，其中spider参数就是被开启的Spider（爬虫）对象</li>
<li>close_spider()：该方法与上一方法相反，是在关闭爬虫时被调用的，在这个方法中可以进行一些收尾工作，其中spider参数就是被关闭的Spider（爬虫）对象</li>
<li>from_crawler()：该方法为类方法，需要使用@classmcthod进行标识，在调用该方法时需要通过参数cls创建实例对象，最后需要返回这个实例对象。通过crawler参数可以获取Scrapy所有的核心组件，例如配置信息等。</li>
</ul>
<h3 id="将信息存储到数据库中"><a href="#将信息存储到数据库中" class="headerlink" title="将信息存储到数据库中"></a>将信息存储到数据库中</h3><p><strong>爬取京东数据并存储至MySQL数据库</strong></p>
<p>了解Item Pipeline（项目管道）的作用后，接下来便可以将爬取的数据信息通过Item Pipeline存储到数据库当中，这里以爬取京东图书排行榜信息为例，将爬取的数据信息存储至MySQL数据库当中。步骤如下：</p>
<ul>
<li>安装并调试MySQL数据库，然后通过Navicat for MySQL创建数据库名称为“jd_data”</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926163826576.png" alt="image-20210926163826576"></p>
<ul>
<li>在“jd_data”数据库当中创建名称为“ranking”的数据表</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926163903681.png" alt="image-20210926163903681"></p>
<ul>
<li>通过谷歌浏览器打开京东图书排行榜网页地址（<a href="https://book.jd.com/booktop/0-0-0.html?category=3287-0-0-0-10001-1），打开开发者工具，选择Elements选项后定位数据位置。" target="_blank" rel="noopener">https://book.jd.com/booktop/0-0-0.html?category=3287-0-0-0-10001-1），打开开发者工具，选择Elements选项后定位数据位置。</a></li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926164103094.png" alt="image-20210926164103094"></p>
<ul>
<li>确定数据位置后，接下来在命令行窗口中通过“<code>scrapy startproject jd</code>”创建名为“jd”的项目结构。然后通过“cd jd”命令打开项目文件夹，最后通过“<code>scrapy genspider jdSpider book.jd.com</code>”命令创建一个jdSpider.py爬虫文件。完整项目结构如下：</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926164352100.png" alt="image-20210926164352100"></p>
<ul>
<li>打开项目文件结构中的items.py文件，在该文件中定义Item</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy

<span class="token keyword">class</span> <span class="token class-name">JdItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># define the fields for your item here like:</span>
    book_name <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 保存图书名称</span>
    author <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 保存作者</span>
    press <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 保存出版社</span>
    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开jdSpider.py爬虫文件，在该文件中<code>重写start_requests()方法</code>，用于实现对京东图书排行榜的网络请求</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 需要访问的地址</span>
    url <span class="token operator">=</span> <span class="token string">'https://book.jd.com/booktop/0-0-0.html?category=3287-0-0-0-10001-1'</span>
    <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span> callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># 发送网络请求</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>在start_requests()方法的下面，<code>重写parse()方法</code>，用于实现网页数据的爬取，然后将爬取的数据添加至Item对象中</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
    all<span class="token operator">=</span>response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">".//*[@class='p-detail']"</span><span class="token punctuation">)</span>                       <span class="token comment" spellcheck="true"># 获取所有信息</span>
    book_name <span class="token operator">=</span> all<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./a[@class='p-name']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># 获取所有图书名称</span>
    author <span class="token operator">=</span> all<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./dl[1]/dd/a[1]/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>              <span class="token comment" spellcheck="true"># 获取所有作者名称</span>
    press <span class="token operator">=</span> all<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"./dl[2]/dd/a/text()"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract<span class="token punctuation">(</span><span class="token punctuation">)</span>                  <span class="token comment" spellcheck="true"># 获取所有出版社名称</span>
    item <span class="token operator">=</span> JdItem<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 创建Item对象</span>
    <span class="token comment" spellcheck="true"># 将数据添加至Item对象</span>
    item<span class="token punctuation">[</span><span class="token string">'book_name'</span><span class="token punctuation">]</span> <span class="token operator">=</span> book_name
    item<span class="token punctuation">[</span><span class="token string">'author'</span><span class="token punctuation">]</span> <span class="token operator">=</span> author
    item<span class="token punctuation">[</span><span class="token string">'press'</span><span class="token punctuation">]</span> <span class="token operator">=</span> press
    <span class="token keyword">yield</span> item    <span class="token comment" spellcheck="true"># 打印item信息</span>
    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>使用PyCharm运行当前爬虫，首先导入CrawlerProcess类与get_project_settings()函数，接着创建程序入口启动爬虫</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 导入CrawlerProcess类</span>
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>crawler <span class="token keyword">import</span> CrawlerProcess
<span class="token comment" spellcheck="true"># 导入获取项目设置信息</span>
<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>project <span class="token keyword">import</span> get_project_settings

<span class="token comment" spellcheck="true"># 程序入口</span>
<span class="token keyword">if</span> __name__<span class="token operator">==</span><span class="token string">'__main__'</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 创建CrawlerProcess类对象并传入项目设置信息参数</span>
    process <span class="token operator">=</span> CrawlerProcess<span class="token punctuation">(</span>get_project_settings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 设置需要启动的爬虫名称</span>
    process<span class="token punctuation">.</span>crawl<span class="token punctuation">(</span><span class="token string">'jdSpider'</span><span class="token punctuation">)</span>
    <span class="token comment" spellcheck="true"># 启动爬虫</span>
    process<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>确认数据已经爬取后，接下来需要在项目管道中将数据存储至MySQL数据库当中，首先打开pipelines.py文件，在该文件中首先导入PyMySQL数据库操作模块，然后通过init()方法初始化数据库连接参数</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> pymysql            <span class="token comment" spellcheck="true"># 导入数据库连接pymysql模块</span>

<span class="token keyword">class</span> <span class="token class-name">JdPipeline</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 初始化数据库参数</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>host<span class="token punctuation">,</span>database<span class="token punctuation">,</span>user<span class="token punctuation">,</span>password<span class="token punctuation">,</span>port<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>host <span class="token operator">=</span> host
        self<span class="token punctuation">.</span>database <span class="token operator">=</span> database
        self<span class="token punctuation">.</span>user <span class="token operator">=</span> user
        self<span class="token punctuation">.</span>password <span class="token operator">=</span> password
        self<span class="token punctuation">.</span>port <span class="token operator">=</span> port<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>重写from_crawler()方法</code>，在该方法中返回通过crawler获取配置文件中数据库参数的cls()实例对象</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python">@classmethod
<span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span>crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 返回cls()实例对象，其中包含通过crawler获取配置文件中的数据库参数</span>
    <span class="token keyword">return</span> cls<span class="token punctuation">(</span>
        host<span class="token operator">=</span>crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'SQL_HOST'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        user<span class="token operator">=</span>crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'SQL_USER'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        password<span class="token operator">=</span>crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'SQL_PASSWORD'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        database <span class="token operator">=</span> crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'SQL_DATABASE'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        port <span class="token operator">=</span> crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'SQL_PORT'</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>重写open_spider()方法</code>，在该方法中实现启动爬虫时进行数据库的连接，以及创建数据库操作游标</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 打开爬虫时调用</span>
<span class="token keyword">def</span> <span class="token function">open_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 数据库连接</span>
    self<span class="token punctuation">.</span>db <span class="token operator">=</span> pymysql<span class="token punctuation">.</span>connect<span class="token punctuation">(</span>self<span class="token punctuation">.</span>host<span class="token punctuation">,</span>self<span class="token punctuation">.</span>user<span class="token punctuation">,</span>self<span class="token punctuation">.</span>password<span class="token punctuation">,</span>self<span class="token punctuation">.</span>database<span class="token punctuation">,</span>self<span class="token punctuation">.</span>port<span class="token punctuation">,</span>charset<span class="token operator">=</span><span class="token string">'utf8'</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>cursor <span class="token operator">=</span> self<span class="token punctuation">.</span>db<span class="token punctuation">.</span>cursor<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#床架游标</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>重写close_spider()方法</code>，在该方法中实现关闭爬虫时关闭数据库的连接</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">close_spider</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>db<span class="token punctuation">.</span>close<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<ul>
<li><code>重写process_item()方法</code>，在该方法中首先将Item对象转换为字典类型的数据，然后将三列数据通过zip()函数转换成每条数据为[(‘book_name’,’press’,’author’)]类型的数据，接着提交并返回Item对象</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">process_item</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    data <span class="token operator">=</span> dict<span class="token punctuation">(</span>item<span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 将item转换成字典类型</span>
    <span class="token comment" spellcheck="true"># sql语句</span>
    sql <span class="token operator">=</span> <span class="token string">'insert into ranking (book_name,press,author) values(%s,%s,%s)'</span>
    <span class="token comment" spellcheck="true"># 执行插入多条数据</span>
    self<span class="token punctuation">.</span>cursor<span class="token punctuation">.</span>executemany<span class="token punctuation">(</span>sql<span class="token punctuation">,</span> list<span class="token punctuation">(</span>zip<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token string">'book_name'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token string">'press'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token string">'author'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>db<span class="token punctuation">.</span>commit<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment" spellcheck="true"># 提交</span>
    <span class="token keyword">return</span> item         <span class="token comment" spellcheck="true"># 返回item</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开settings.py文件，在该文件中找到激活项目管道的代码并解除注释状态，然后设置数据库信息的变量。</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Configure item pipelines</span>
<span class="token comment" spellcheck="true"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span>
<span class="token comment" spellcheck="true"># 配置数据库连接信息</span>
SQL_HOST <span class="token operator">=</span> <span class="token string">'localhost'</span>
SQL_USER <span class="token operator">=</span> <span class="token string">'root'</span>
SQL_PASSWORD<span class="token operator">=</span><span class="token string">'root'</span>
SQL_DATABASE <span class="token operator">=</span> <span class="token string">'jd_data'</span>
SQL_PORT <span class="token operator">=</span> <span class="token number">3306</span>
<span class="token comment" spellcheck="true"># 开启jd项目管道</span>
ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{</span>
   <span class="token string">'jd.pipelines.JdPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开jdSpider.py文件，在该文件中再次启动爬虫，爬虫程序执行完毕后，打开ranking数据表，将显示如下数据信息</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926165816888.png" alt="image-20210926165816888"></p>
<h2 id="自定义中间件"><a href="#自定义中间件" class="headerlink" title="自定义中间件"></a>自定义中间件</h2><p>Scrapy中内置了多个中间件，不过多数情况下开发者会选择创建一个属于自己的中间件，这样既可以满足自己的开发需求，还可以节省很多开发时间。<code>在实现自定义中间件时需要重写部分方法</code>，因为<code>Scrapy引擎需要根据这些方法名来执行并处理</code>，如果没有重写这些方法，Scrapy的引擎将会按照原有的方法执行，从而失去自定义中间件的意义。</p>
<h3 id="设置随机请求头"><a href="#设置随机请求头" class="headerlink" title="设置随机请求头"></a>设置随机请求头</h3><p>设置请求头是爬虫程序中必不可少的一项设置，大多数网站都会根据请求头内容制订一些反爬策略，在Scrapy框架中如果只是简单地设置一个请求头的话，可以在当前的爬虫文件中以参数的形式添加在网络请求当中。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy    <span class="token comment" spellcheck="true"># 导入框架</span>

<span class="token keyword">class</span> <span class="token class-name">HeaderSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'header'</span>                  <span class="token comment" spellcheck="true"># 定义爬虫名称</span>

    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 设置固定的请求头</span>
        self<span class="token punctuation">.</span>headers <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">'User-Agent'</span><span class="token punctuation">:</span><span class="token string">'Mozilla/5.0 (Windows NT 10.0;'</span>
                        <span class="token string">'Win64; x64; rv:74.0) Gecko/20100101 Firefox/74.0'</span><span class="token punctuation">}</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span>scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span><span class="token string">'http://httpbin.org/get'</span><span class="token punctuation">,</span>headers<span class="token operator">=</span>self<span class="token punctuation">.</span>headers<span class="token punctuation">,</span>callbcak<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token comment" spellcheck="true"># 响应信息</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印返回的响应信息</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>程序执行结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926222207836.png" alt="image-20210926222207836"></p>
<blockquote>
<p>在没有使用指定的请求头时，发送网络请求将使用Scrapy默认的请求头信息，信息内容如下：</p>
<p>“User-Agent”: “Scrapy/1.6.0 (+<a href="https://scrapy.org)&quot;" target="_blank" rel="noopener">https://scrapy.org)&quot;</a></p>
</blockquote>
<p>对于实现多个网络请求时，最好每发送一次请求就更换一个请求头，这样可以避免请求头的反爬策略。对于这样的需求可以使用自定义中间件的方式实现一个设置随机请求头的中间件。具体步骤如下：</p>
<ul>
<li>打开命令行窗口，首先通过“<code>scrapy startproject header</code>”命令创建一个名为“header”的项目，然后通过“<code>cd header</code>”命令打开项目最外层的文件夹，最后通过“<code>scrapy genspider headerSpider quotes.toscrape.com</code>”命令创建名称为“headerSpider”的爬虫文件。</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926223251499.png" alt="image-20210926223251499"></p>
<ul>
<li>打开headerSpider.py文件，配置测试网络请求的爬虫代码</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">HeaderspiderSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'headerSpider'</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'quotes.toscrape.com'</span><span class="token punctuation">]</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://quotes.toscrape.com/'</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 设置爬取目标的地址</span>
        urls <span class="token operator">=</span> <span class="token punctuation">[</span>
            <span class="token string">'http://quotes.toscrape.com/page/1/'</span><span class="token punctuation">,</span>
            <span class="token string">'http://quotes.toscrape.com/page/2/'</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span>

        <span class="token comment" spellcheck="true"># 获取所有地址，有几个地址发送几次请求</span>
        <span class="token keyword">for</span> url <span class="token keyword">in</span> urls<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># 发送网络请求</span>
            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>url<span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 打印每次网络请求的请求头信息</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'请求信息为：'</span><span class="token punctuation">,</span>response<span class="token punctuation">.</span>request<span class="token punctuation">.</span>headers<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'User-Agent'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>安装fake-useragent模块，打开<code>middlewares.py文件</code>，在该文件中首先导入fake-useragent模块中的UserAgent类，然后创建RandomHeaderMiddleware类并通过init()函数进行类的初始化工作</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> fake_useragent <span class="token keyword">import</span> UserAgent  <span class="token comment" spellcheck="true"># 导入请求头类</span>
<span class="token comment" spellcheck="true"># 自定义随机请求头的中间件</span>
<span class="token keyword">class</span> <span class="token class-name">RandomHeaderMiddleware</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>ua <span class="token operator">=</span> UserAgent<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 随机请求头对象</span>
        <span class="token comment" spellcheck="true"># 如果配置文件中不存在就使用默认的Google Chrome请求头</span>
        self<span class="token punctuation">.</span>type <span class="token operator">=</span> crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"RANDOM_UA_TYPE"</span><span class="token punctuation">,</span> <span class="token string">"chrome"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>重写from_crawler()方法</code>，在该方法中将cls()实例对象返回</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python">@classmethod
<span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 返回cls()实例对象</span>
    <span class="token keyword">return</span> cls<span class="token punctuation">(</span>crawler<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>重写process_request()方法</code>，在该方法中实现设置随机生成的请求头信息</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#发送网络请求时调用该方法</span>
<span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 设置随机生成的请求头</span>
    request<span class="token punctuation">.</span>headers<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span><span class="token string">'User-Agent'</span><span class="token punctuation">,</span>getattr<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ua<span class="token punctuation">,</span> self<span class="token punctuation">.</span>type<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开settings.py文件，在该文件中找到DOWNLOADER_MIDDLEWARES配置信息，然后配置自定义的请求头中间件，并把默认生成的下载中间件禁用，最后在配置信息的下面添加请求头类型</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Enable or disable downloader middlewares</span>
<span class="token comment" spellcheck="true"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span>

DOWNLOADER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token comment" spellcheck="true"># 启动自定义随机请求头中间件</span>
   <span class="token string">'header.middlewares.RandomHeaderMiddleware'</span><span class="token punctuation">:</span><span class="token number">400</span><span class="token punctuation">,</span>
    <span class="token comment" spellcheck="true"># 设为None，禁用默认创建的下载中间件</span>
   <span class="token string">'header.middlewares.HeaderDownloaderMiddleware'</span><span class="token punctuation">:</span> None<span class="token punctuation">,</span>
<span class="token punctuation">}</span>
<span class="token comment" spellcheck="true"># 配置请求头类型为随机，此处还可以设置为ie、firefox以及chrome</span>
RANDOM_UA_TYPE <span class="token operator">=</span> <span class="token string">"random"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>启动“headerSpider”爬虫，控制台将输出两次请求，分别使用不同的请求头信息</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926224943525.png" alt="image-20210926224943525"></p>
<blockquote>
<p>本次自定义中间件中的重点是重写process_request()方法，该方法是Scrapy发送网络请求时所调用的，参数request表示当前的请求对象，例如请求头、请求方式以及请求地址等信息。参数spider表示爬虫程序。该方法返回值的具体说明如下：</p>
</blockquote>
<ul>
<li>None：最常见的返回值，表示该方法已经执行完成并向下执行爬虫程序</li>
<li>response：停止该方法的执行，开始执行process_response()方法</li>
<li>request：停止当前的中间件，将当前的请求交给Scrapy引擎重新执行</li>
<li>IgnoreRequest：抛出异常对象，再通过process_exception()方法处理异常，结束当前的网络请求</li>
</ul>
<h3 id="设置Cookies"><a href="#设置Cookies" class="headerlink" title="设置Cookies"></a>设置Cookies</h3><p>Cookies代表着用户信息，如果需要爬取登录后网页的信息时，就可以将Cookies信息保存，然后在第二次获取登录后的信息时就不需要再次登录了，直接使用Cookies进行登录即可。在Scrapy中，如果想在Spider（爬虫）文件中直接定义并设置Cookies参数时，可以参考以下代码</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy

<span class="token keyword">class</span> <span class="token class-name">CookiespiderSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'cookieSpider'</span>                    <span class="token comment" spellcheck="true"># 爬虫名称</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'httpbin.org/get'</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 域名列表</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://httpbin.org/get'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 请求初始化列表</span>
    cookies <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'CookiesDemo'</span><span class="token punctuation">:</span><span class="token string">'python'</span><span class="token punctuation">]</span>      <span class="token comment" spellcheck="true"># 模拟Cookies信息</span>

    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 发送网络请求，请求地址为start_urls列表中的第一个地址</span>
        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>self<span class="token punctuation">.</span>start_urls<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>cookies<span class="token operator">=</span>self<span class="token punctuation">.</span>cookies<span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># 响应信息</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 打印响应结果</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>
        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>程序执行结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926230647715.png" alt="image-20210926230647715"></p>
<blockquote>
<p>上述Cookies是一个模拟测试使用的信息，并不是真实有效的Cookies信息，所以在使用时需要将Cookies信息设置为爬取网站对应的真实Cookies</p>
</blockquote>
<p>在Scrapy中除了使用以上示例代码中的方法设置Cookies以外，也可以使用自定义中间件的方式设置Cookies。以爬取某网站登录后的用户名信息为例，具体步骤如下：</p>
<ul>
<li>在cookieSpider.py文件中编写爬虫代码</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy

<span class="token keyword">class</span> <span class="token class-name">CookiespiderSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'cookieSpider'</span>                    <span class="token comment" spellcheck="true"># 爬虫名称</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'douban.com'</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 域名列表</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://www.douban.com'</span><span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># 请求初始化列表</span>

    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 发送网络请求，请求地址为start_urls列表中的第一个地址</span>
        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token operator">=</span>self<span class="token punctuation">.</span>start_urls<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># 响应信息</span>
    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 打印登录后的用户名信息</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">'//*[@id="db-global-nav"]/div/div[1]/ul/li[2]/a/span[1]/text()'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>extract_first<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>在middlewares.py文件中，定义用于格式化与设置Cookies的中间件</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 自定义cookies中间件</span>
<span class="token keyword">class</span> <span class="token class-name">CookiesdemoMiddleware</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 初始化</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>cookies_str<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>cookies_str <span class="token operator">=</span> cookies_str

    @classmethod
    <span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> cls<span class="token punctuation">(</span>
            <span class="token comment" spellcheck="true"># 获取配置文件中的cookies信息</span>
            cookies_str <span class="token operator">=</span> crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'COOKIES_DEMO'</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
    cookies <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>    <span class="token comment" spellcheck="true"># 保存格式化以后的cookies</span>
    <span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> cookie <span class="token keyword">in</span> self<span class="token punctuation">.</span>cookies_str<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">';'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 通过；分割cookies字符串</span>
            key<span class="token punctuation">,</span> value <span class="token operator">=</span> cookie<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'='</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true"># 将key与值进行分割</span>
            self<span class="token punctuation">.</span>cookies<span class="token punctuation">.</span>__setitem__<span class="token punctuation">(</span>key<span class="token punctuation">,</span>value<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 将分割后的数据保存至字典中</span>
        request<span class="token punctuation">.</span>cookies <span class="token operator">=</span> self<span class="token punctuation">.</span>cookies                 <span class="token comment" spellcheck="true"># 设置格式化以后的cookies</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>在middlewares.py文件中，定义随机设置请求头的中间件</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> fake_useragent <span class="token keyword">import</span> UserAgent  <span class="token comment" spellcheck="true"># 导入请求头类</span>
<span class="token comment" spellcheck="true"># 自定义随机请求头的中间件</span>
<span class="token keyword">class</span> <span class="token class-name">RandomHeaderMiddleware</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>ua <span class="token operator">=</span> UserAgent<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 随机请求头对象</span>
        <span class="token comment" spellcheck="true"># 如果配置文件中不存在就使用默认的Google Chrome请求头</span>
        self<span class="token punctuation">.</span>type <span class="token operator">=</span> crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"RANDOM_UA_TYPE"</span><span class="token punctuation">,</span> <span class="token string">"chrome"</span><span class="token punctuation">)</span>

    @classmethod
    <span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 返回cls()实例对象</span>
        <span class="token keyword">return</span> cls<span class="token punctuation">(</span>crawler<span class="token punctuation">)</span>

    <span class="token comment" spellcheck="true"># 发送网络请求时调用该方法</span>
    <span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 设置随机生成的请求头</span>
        request<span class="token punctuation">.</span>headers<span class="token punctuation">.</span>setdefault<span class="token punctuation">(</span><span class="token string">'User-Agent'</span><span class="token punctuation">,</span> getattr<span class="token punctuation">(</span>self<span class="token punctuation">.</span>ua<span class="token punctuation">,</span> self<span class="token punctuation">.</span>type<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开settings.py文件，在该文件中首先将DOWNLOADER_MIDDLEWARES配置信息中的默认配置信息禁用，然后添加用于处理Cookies与随机请求头的配置信息并激活，最后定义从浏览器中获取的Cookies信息</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python">DOWNLOADER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token comment" spellcheck="true"># 启动自定义cookies中间件</span>
    <span class="token string">'cookiesDemo.middlewares.CookiesdemoMiddleware'</span><span class="token punctuation">:</span> <span class="token number">201</span><span class="token punctuation">,</span>
    <span class="token comment" spellcheck="true"># 启动自定义随机请求头中间件</span>
   <span class="token string">'cookiesDemo.middlewares.RandomHeaderMiddleware'</span><span class="token punctuation">:</span><span class="token number">202</span><span class="token punctuation">,</span>
    <span class="token comment" spellcheck="true"># 禁用默认生成的配置信息</span>
   <span class="token string">'cookiesDemo.middlewares.CookiesdemoDownloaderMiddleware'</span><span class="token punctuation">:</span> None<span class="token punctuation">,</span>
<span class="token punctuation">}</span>
<span class="token comment" spellcheck="true"># 定义从浏览器中获取的Cookies</span>
COOKIES_DEMO <span class="token operator">=</span> <span class="token string">'此处填写登录后网页中的Cookie信息'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>程序执行结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926232309591.png" alt="image-20210926232309591"></p>
<h3 id="设置代理IP"><a href="#设置代理IP" class="headerlink" title="设置代理IP"></a>设置代理IP</h3><p>使用代理IP实现网络爬虫是有效解决反爬虫的一种方法，如果只是想在Scrapy中简单地应用一次代理IP时可以使用以下代码：</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#发送网络请求</span>
<span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span><span class="token string">'http://httpbin.org/get'</span><span class="token punctuation">,</span>callback <span class="token operator">=</span> self<span class="token punctuation">.</span>parse<span class="token punctuation">,</span>meta<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'proxy'</span><span class="token punctuation">:</span><span class="token string">'http://117.88.177.0:3000'</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token comment" spellcheck="true"># 响应信息</span>
<span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印返回的响应信息</span>
    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>程序执行结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926232802039.png" alt="image-20210926232802039"></p>
<blockquote>
<p>在使用代理IP发送网络请求时，需要确保代理IP是一个有效的IP，否则会出现错误</p>
</blockquote>
<p><strong>随机代理IP中间件实现网络请求</strong></p>
<p>如果需要发送多个网络请求时，可以自定义一个代理IP的中间件，在这个中间件中使用随机的方式从代理IP列表内随机抽取一个有效的代理IP，并通过这个有效的代理IP实现网络请求，具体步骤如下：</p>
<ul>
<li>在ipSpider.py文件中编写爬虫代码</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#发送网络请求</span>
<span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span><span class="token string">'http://httpbin.org/get'</span><span class="token punctuation">,</span>callback <span class="token operator">=</span> self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token comment" spellcheck="true"># 响应信息</span>
<span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印返回的响应信息</span>
    <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开middlewares.py文件，在该文件中创建IpdemoProxyMiddleware类，然后定义保存代理IP的列表，最后重写process_request()方法在该方法中实现发送网络请求时随机抽取有效的代理IP</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> random   <span class="token comment" spellcheck="true"># 导入随机模块</span>

<span class="token keyword">class</span> <span class="token class-name">IpRandomProxyMiddleware</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment" spellcheck="true"># 定义有效的代理ip列表</span>
    PROXIES <span class="token operator">=</span> <span class="token punctuation">[</span>
        <span class="token string">'117.88.177.0:3000'</span><span class="token punctuation">,</span>
        <span class="token string">'117.45.139.179:9006'</span><span class="token punctuation">,</span>
        <span class="token string">'202.115.142.147:9200'</span><span class="token punctuation">,</span>
        <span class="token string">'117.87.50.89:8118'</span><span class="token punctuation">]</span>
    <span class="token comment" spellcheck="true"># 发送网络请求时调用</span>
    <span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
        proxy <span class="token operator">=</span> random<span class="token punctuation">.</span>choice<span class="token punctuation">(</span>self<span class="token punctuation">.</span>PROXIES<span class="token punctuation">)</span>          <span class="token comment" spellcheck="true"># 随机抽取代理ip</span>
        request<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'proxy'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://'</span><span class="token operator">+</span>proxy      <span class="token comment" spellcheck="true"># 设置网络请求所使用的代理ip</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>在settings.py文件中修改DOWNLOADER_MIDDLEWARES配置信息，先将默认生成的配置信息禁用，然后激活自定义随机获取代理IP的中间件</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Enable or disable downloader middlewares</span>
<span class="token comment" spellcheck="true"># See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</span>

DOWNLOADER_MIDDLEWARES <span class="token operator">=</span> <span class="token punctuation">{</span>
    <span class="token comment" spellcheck="true"># 激活自定义随机获取代理ip的中间件</span>
    <span class="token string">'ipDemo.middlewares.IpRandomProxyMiddleware'</span><span class="token punctuation">:</span><span class="token number">200</span><span class="token punctuation">,</span>
    <span class="token comment" spellcheck="true"># 禁用默认生成的中间件</span>
   <span class="token string">'ipDemo.middlewares.IpdemoDownloaderMiddleware'</span><span class="token punctuation">:</span> None
<span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>程序执行结果：</p>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926234039759.png" alt="image-20210926234039759"></p>
<h2 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h2><p>Scrapy中提供了可以专门处理下载的Pipeline（项目管道），其中包括Files Pipeline（文件管道）以及Images Pipeline（图像管道）。两种项目管道的使用方式相同，只是在使用Images Pipeline（图像管道）时可以将所有下载的图片格式转换为JPEG/RGB格式以及设置缩略图。</p>
<p>以继承ImagesPipeline类为例，可以重写以下三个方法：</p>
<ul>
<li>file_path()：该方法用于返回指定文件名的下载路径，第一个request参数是当前下载对应的request对象</li>
<li>get_media_requests()：该方法中的第一个参数为Item对象，这里可以通过Item获取URL，然后将URL加入请求队列，等待请求</li>
<li>item_completed()：当单个Item完成下载后的处理方法，通过该方法可以实现筛选下载失败的图片。该方法中的一个参数results就是当前Item对应的下载结果，其中包含下载成功或失败的信息</li>
</ul>
<p><strong>下载京东外设的商品图片</strong></p>
<p>具体步骤如下：</p>
<ul>
<li>在命令行窗口中通过命令创建名称为“imagesDemo”的Scrapy项目，然后在该项目中的spiders文件夹内创建imagesSpider.py爬虫文件，接着打开items.py文件，在该文件中创建存储商品名称与图片地址的Field()对象。</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> scrapy      <span class="token comment" spellcheck="true"># 导入scrapy模块</span>

<span class="token keyword">class</span> <span class="token class-name">ImagesdemoItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>
    wareName <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 存储商品名称</span>
    imgPath <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 存储商品图片地址</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开“imagesSpider.py”文件，在该文件中首先导入json模块，然后重写start_requests()方法实现获取json信息的网络请求，最后重写parse()方法，在该方法中实现商品名称与图片地址的提取</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># -*- coding: utf-8 -*-</span>
<span class="token keyword">import</span> scrapy       <span class="token comment" spellcheck="true"># 导入scrapy模块</span>
<span class="token keyword">import</span> json         <span class="token comment" spellcheck="true"># 导入json模块</span>
<span class="token comment" spellcheck="true"># 导入ImagesdemoItem类</span>
<span class="token keyword">from</span> imagesDemo<span class="token punctuation">.</span>items <span class="token keyword">import</span> ImagesdemoItem
<span class="token keyword">class</span> <span class="token class-name">ImgesspiderSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name <span class="token operator">=</span> <span class="token string">'imgesSpider'</span>                <span class="token comment" spellcheck="true"># 爬虫名称</span>
    allowed_domains <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'ch.jd.com'</span><span class="token punctuation">]</span>     <span class="token comment" spellcheck="true"># 域名列表</span>
    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'http://ch.jd.com/'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 网络请求初始列表</span>

    <span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        url <span class="token operator">=</span> <span class="token string">'http://ch.jd.com/hotsale2?cateid=686'</span>  <span class="token comment" spellcheck="true"># 获取json信息的请求地址</span>
        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token punctuation">,</span> self<span class="token punctuation">.</span>parse<span class="token punctuation">)</span>         <span class="token comment" spellcheck="true"># 发送网络请求</span>

    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>
        data <span class="token operator">=</span> json<span class="token punctuation">.</span>loads<span class="token punctuation">(</span>response<span class="token punctuation">.</span>text<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 将返回的json信息转换为字典</span>
        products <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token string">'products'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 获取所有数据信息</span>
        <span class="token keyword">for</span> image <span class="token keyword">in</span> products<span class="token punctuation">:</span>  <span class="token comment" spellcheck="true"># 循环遍历信息</span>
            item <span class="token operator">=</span> ImagesdemoItem<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 创建item对象</span>
            item<span class="token punctuation">[</span><span class="token string">'wareName'</span><span class="token punctuation">]</span> <span class="token operator">=</span> image<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'wareName'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">,</span><span class="token string">''</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 存储商品名称</span>
            <span class="token comment" spellcheck="true"># 存储商品对应的图片地址</span>
            item<span class="token punctuation">[</span><span class="token string">'imgPath'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://img12.360buyimg.com/n1/s320x320_'</span> <span class="token operator">+</span> image<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'imgPath'</span><span class="token punctuation">)</span>
            <span class="token keyword">yield</span> item<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>打开pipelines.py文件，在该文件中首先要导入ImagesPipeline类，然后让自己定义的类继承自ImagesPipeline类。接着<code>重写file_path()方法</code>与<code>get_media_requests()方法</code>，分别用于设置图片文件的名称以及发送获取图片的网络请求</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>pipelines<span class="token punctuation">.</span>images <span class="token keyword">import</span> ImagesPipeline   <span class="token comment" spellcheck="true"># 导入ImagesPipeline类</span>
<span class="token keyword">import</span> scrapy                                         <span class="token comment" spellcheck="true"># 导入scrapy</span>
<span class="token keyword">class</span> <span class="token class-name">ImagesdemoPipeline</span><span class="token punctuation">(</span>ImagesPipeline<span class="token punctuation">)</span><span class="token punctuation">:</span>             <span class="token comment" spellcheck="true"># 继承ImagesPipeline类</span>
    <span class="token comment" spellcheck="true"># 设置文件保存的名称</span>
    <span class="token keyword">def</span> <span class="token function">file_path</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> response<span class="token operator">=</span>None<span class="token punctuation">,</span> info<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>
        file_name <span class="token operator">=</span> request<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'name'</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token string">'.jpg'</span>  <span class="token comment" spellcheck="true"># 将商品名称设置为图片名称</span>
        <span class="token keyword">return</span> file_name                         <span class="token comment" spellcheck="true"># 返回文件名称</span>

    <span class="token comment" spellcheck="true"># 发送获取图片的网络请求</span>
    <span class="token keyword">def</span> <span class="token function">get_media_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> info<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># 发送网络请求并传递商品名称</span>
        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>item<span class="token punctuation">[</span><span class="token string">'imgPath'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>meta<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'name'</span><span class="token punctuation">:</span>item<span class="token punctuation">[</span><span class="token string">'wareName'</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>在settings.py文件中激活ITEM_PIPELINES配置信息，然后再下面定义IMAGES_STORE变量并指定图片下载后所保存的位置</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Configure item pipelines</span>
<span class="token comment" spellcheck="true"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span>

ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">{</span>
   <span class="token comment" spellcheck="true"># 激活下载京东商品图片的管道</span>
   <span class="token string">'imagesDemo.pipelines.ImagesdemoPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
IMAGES_STORE <span class="token operator">=</span> <span class="token string">'./images'</span>    <span class="token comment" spellcheck="true"># 此处的路径变量名称必须是固定的IMAGES_STORE</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>启动“imagesSpider”爬虫，下载完成后，打开项目结构中的images文件夹将显示下图商品图片</li>
</ul>
<p><img src="https://gitee.com/charlesli1111/picgo-image-lib/raw/master/image-20210926235803316.png" alt="image-20210926235803316"></p>

            </div>
            <hr />

            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>转载规则</span>
        </p>
        
            <div class="center-align">
                <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    《Scrapy爬虫框架》
                </span> 由
            <a xmlns:cc="http://creativecommons.org/ns#" href="/scrapy-pa-chong-kuang-jia.html" property="cc:attributionName"
               rel="cc:attributionURL">
                CharlesLi
            </a> 采用
            <a rel="license" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                知识共享署名 4.0 国际许可协议
            </a>进行许可。
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    
    <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '',
        clientSecret: '',
        repo: 'charles-li-github.github.io',
        owner: 'charles-li-github',
        admin: "charles-li-github",
        id: 'scrapy-pa-chong-kuang-jia.html',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    
    <style>
    .valine-card {
        margin: 1.5rem auto;
    }

    .valine-card .card-content {
        padding: 20px 20px 5px 20px;
    }

    #vcomments input[type=text],
    #vcomments input[type=email],
    #vcomments input[type=url],
    #vcomments textarea {
        box-sizing: border-box;
    }

    #vcomments p {
        margin: 2px 2px 10px;
        font-size: 1.05rem;
        line-height: 1.78rem;
    }

    #vcomments blockquote p {
        text-indent: 0.2rem;
    }

    #vcomments a {
        padding: 0 2px;
        color: #42b983;
        font-weight: 500;
        text-decoration: underline;
    }

    #vcomments img {
        max-width: 100%;
        height: auto;
        cursor: pointer;
    }

    #vcomments ol li {
        list-style-type: decimal;
    }

    #vcomments ol,
    ul {
        display: block;
        padding-left: 2em;
        word-spacing: 0.05rem;
    }

    #vcomments ul li,
    ol li {
        display: list-item;
        line-height: 1.8rem;
        font-size: 1rem;
    }

    #vcomments ul li {
        list-style-type: disc;
    }

    #vcomments ul ul li {
        list-style-type: circle;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    #vcomments table, th, td {
        border: 0;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments h1 {
        font-size: 1.85rem;
        font-weight: bold;
        line-height: 2.2rem;
    }

    #vcomments h2 {
        font-size: 1.65rem;
        font-weight: bold;
        line-height: 1.9rem;
    }

    #vcomments h3 {
        font-size: 1.45rem;
        font-weight: bold;
        line-height: 1.7rem;
    }

    #vcomments h4 {
        font-size: 1.25rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    #vcomments h5 {
        font-size: 1.1rem;
        font-weight: bold;
        line-height: 1.4rem;
    }

    #vcomments h6 {
        font-size: 1rem;
        line-height: 1.3rem;
    }

    #vcomments p {
        font-size: 1rem;
        line-height: 1.5rem;
    }

    #vcomments hr {
        margin: 12px 0;
        border: 0;
        border-top: 1px solid #ccc;
    }

    #vcomments blockquote {
        margin: 15px 0;
        border-left: 5px solid #42b983;
        padding: 1rem 0.8rem 0.3rem 0.8rem;
        color: #666;
        background-color: rgba(66, 185, 131, .1);
    }

    #vcomments pre {
        font-family: monospace, monospace;
        padding: 1.2em;
        margin: .5em 0;
        background: #272822;
        overflow: auto;
        border-radius: 0.3em;
        tab-size: 4;
    }

    #vcomments code {
        font-family: monospace, monospace;
        padding: 1px 3px;
        font-size: 0.92rem;
        color: #e96900;
        background-color: #f8f8f8;
        border-radius: 2px;
    }

    #vcomments pre code {
        font-family: monospace, monospace;
        padding: 0;
        color: #e8eaf6;
        background-color: #272822;
    }

    #vcomments pre[class*="language-"] {
        padding: 1.2em;
        margin: .5em 0;
    }

    #vcomments code[class*="language-"],
    pre[class*="language-"] {
        color: #e8eaf6;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }

    #vcomments b,
    strong {
        font-weight: bold;
    }

    #vcomments dfn {
        font-style: italic;
    }

    #vcomments small {
        font-size: 85%;
    }

    #vcomments cite {
        font-style: normal;
    }

    #vcomments mark {
        background-color: #fcf8e3;
        padding: .2em;
    }

    #vcomments table, th, td {
        padding: 12px 13px;
        border: 1px solid #dfe2e5;
    }

    table tr:nth-child(2n), thead {
        background-color: #fafafa;
    }

    #vcomments table th {
        background-color: #f2f2f2;
        min-width: 80px;
    }

    #vcomments table td {
        min-width: 80px;
    }

    #vcomments [type="checkbox"]:not(:checked), [type="checkbox"]:checked {
        position: inherit;
        margin-left: -1.3rem;
        margin-right: 0.4rem;
        margin-top: -1px;
        vertical-align: middle;
        left: unset;
        visibility: visible;
    }
</style>

<div class="card valine-card" data-aos="fade-up">
    <div id="vcomments" class="card-content"></div>
</div>

<script src="/libs/valine/av-min.js"></script>
<script src="/libs/valine/Valine.min.js"></script>
<!-- <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script> -->

<script>
    new Valine({
        el: '#vcomments',
        appId: '',
        appKey: '',
        notify: 'true' === 'true',
        verify: 'false' === 'true',
        visitor: 'false' === 'true',
        avatar: 'wavatar',
        pageSize: '10',
        lang: 'zh-cn',
        placeholder: '如果你没有GitHub账号，还可以在这里评论啦！'
    });
</script>

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-dot-circle-o"></i>&nbsp;本篇
            </div>
            <div class="card">
                <a href="/scrapy-pa-chong-kuang-jia.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="Scrapy爬虫框架">
                        
                        <span class="card-title">Scrapy爬虫框架</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Python爬虫常用模块之Scrapy爬虫框架
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2021-09-26
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/网络爬虫/" class="post-category" target="_blank">
                                    网络爬虫
                                </a>
                            
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/python/" target="_blank">
                        <span class="chip bg-color">python</span>
                    </a>
                    
                    <a href="/tags/Scrapy/" target="_blank">
                        <span class="chip bg-color">Scrapy</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/shi-bie-yan-zheng-ma.html">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="识别验证码">
                        
                        <span class="card-title">识别验证码</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Python爬虫常用模块之识别验证码
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2021-09-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/网络爬虫/" class="post-category" target="_blank">
                                    网络爬虫
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/python/" target="_blank">
                        <span class="chip bg-color">python</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Charles Blog<br />'
            + '作者: CharlesLi<br />'
            + '链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () { bodyElement.removeChild(newdiv); }, 200);
    });
</script>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>
<!-- 代码语言 -->
<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>
<!-- 代码块复制 -->
<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>
<script type="text/javascript" src="/libs/codeBlock/clipboard.min.js"></script>
<!-- 代码块收缩 -->
<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script> 
<!-- 代码块折行 -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
       <div class="Copy-right">
            &copy; 202i Charles. All Rights Reserved.

                    
           <!--<a href="/sitemap.xml" target="_blank">站点地图</a>丨-->
           
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
            <span class="white-color">176.9k
             丨            
          

            <span id="sitetime"></span>

            
            
            
            
            <span id="busuanzi_container_site_pv" style='display:none'>
                <i class="fa fa-users"></i>
                本站总访问量 <span id="busuanzi_value_site_pv" class="red-color"></span>
            </span>
            
            
            <span id="busuanzi_container_site_uv" style='display:none'>
               人次,&nbsp;<i class="fa fa-user"></i>访客数 <span id="busuanzi_value_site_uv" class="blue-color"></span> 人.
            </span>
            
            
        </div>        
        <div class="social-link social-statis">
    <a href="https://github.com//" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="https://www.csdn.net//" class="tooltipped" target="_blank" data-tooltip="访问我的CSDN主页" data-position="top" data-delay="50">
        <i class="fa fa-codiepie"></i>
    </a>



    <a href="https://www.zhihu.com//" class="tooltipped" target="_blank" data-tooltip="访问我的知乎" data-position="top" data-delay="50">
        <i class="fa fa-inverse">知</i>
    </a>



    <a href="https://user.qzone.qq.com//" class="tooltipped" target="_blank" data-tooltip="访问我的QQ空间" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>



    <a href="mailto:hangliccclh@gmail.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>
</div>
    </div>
    <div class="container row center-align">
    <div class="github-badge">
      <a style="color: #fff" rel="license" href="https://hexo.io/" target="_blank" title="由 Hexo 强力驱动">
      <span class="badge-subject">Powered</span><span class="badge-value bg-blue">Hexo</span></a>
    </div>
    <div class="github-badge">
      <a style="color: #fff" rel="license" href="https://github.com/" target="_blank" title="静态网页托管于 GitHub Pages 和 Coding Pages">
      <span class="badge-subject">Hosted</span><span class="badge-value bg-brightgreen">GitHub & Coding</span></a>
    </div>
    <div class="github-badge">
      <a style="color: #fff" rel="license" href="https://www.cloud.tencent.com/" target="_blank" title="腾讯云提供域名相关服务">
      <span class="badge-subject">DNS</span><span class="badge-value bg-blueviolet">Tencent cloud</span></a>
    </div>
    <div class="github-badge">
      <a style="color: #fff" rel="license" href="https://www.jsdelivr.com/" target="_blank" title="jsDelivr 提供 CDN 加速服务">
      <span class="badge-subject">CDN</span><span class="badge-value bg-orange">jsDelivr</span></a>
    </div>
    <div class="github-badge">
        <a style="color: #fff" rel="license" href="https://github.com/blinkfox/hexo-theme-matery/" target="_blank" title="站点使用 Matery 主题">
      <span class="badge-subject">Theme</span><span class="badge-value bg-blue">Matery</span></a>
    </div>
    <div class="github-badge">
      <a style="color: #fff" rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" title="本站点采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议进行许可">
      <span class="badge-subject"><i class="fa fa-copyright"></i></span><span class="badge-value bg-lightgrey">BY-NC-SA 4.0</span></a>
    </div>
    <div class="github-badge">
      <a style="color: #fff" rel="license" href="https://996.icu/" target="_blank" title="支持 996.ICU">
      <span class="badge-subject">Link</span><span class="badge-value bg-red">996.ICU</span></a>
    </div>
    <div class="github-badge">
      <span class="badge-subject">UV</span><span class="badge-value bg-orange" id="busuanzi_value_site_uv"></span>
    </div>
    <div class="github-badge">
      <span class="badge-subject">PV</span><span class="badge-value bg-brightgreen" id="busuanzi_value_site_pv"></span>
    </div>
    <div class="github-badge">
      <span class="badge-subject">WordCount</span><span class="badge-value bg-blueviolet">176.9k</span>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- 不蒜子计数初始值纠正 -->
<script>
    $(document).ready(function () {

        var int = setInterval(fixCount, 50);
        var pvcountOffset = 0;
        var uvcountOffset = 0;

        function fixCount() {
            if (document.getElementById("busuanzi_container_site_pv").style.display != "none") {
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + pvcountOffset);
                clearInterval(int);
            }
            if ($("#busuanzi_container_site_pv").css("display") != "none") {
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + uvcountOffset); // 加上初始数据 
                clearInterval(int);
            }
        }
    });
</script>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- 返回date对象距世界标准时间(UTC)1970年1月1日午夜之间的毫秒数(时间戳)
        year - 作为date对象的年份，为4位年份值
        month - 0-11之间的整数，做为date对象的月份
        day - 1-31之间的整数，做为date对象的天数
        hours - 0(午夜24点)-23之间的整数，做为date对象的小时数
        minutes - 0-59之间的整数，做为date对象的分钟数
        seconds - 0-59之间的整数，做为date对象的秒数
        microseconds - 0-999之间的整数，做为date对象的毫秒数 */
        var t1 = Date.UTC(2021, 08, 18, 00, 00, 00); //北京时间2018-2-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "本站已运行 " + diffYears + " 年 " + diffDays + " 天 " + diffHours + " 小时 " + diffMinutes + " 分钟 " + diffSeconds + " 秒";
    }/*因为建站时间还没有一年，就将之注释掉了。需要的可以取消*/
    siteTime();
</script>

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <script type="text/javascript"> var OriginTitile = document.title, st; document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Σ(っ °Д °;)っ喔哟，崩溃啦！", clearTimeout(st)) : (document.title = "φ(゜▽゜*)♪咦，又好了！", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) })
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->

<script async src="https://www.googletagmanager.com/gtag/js?id="></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
        dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', '');
</script>



    
    <script src="/libs/others/clicklove.js"></script>
    

    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    <!-- 雪花特效 -->
    
    <script type="text/javascript" src="/libs/others/snow.js"></script>
    

   <!-- 单击显示文字 -->
   <!-- <script type="text/javascript" src="/js/click_show_text.js"></script>  -->

   <!--动态线条背景-->
   <script type="text/javascript"
   color="220,220,220" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
   </script>

   <script async src =“ https://cdn.jsdelivr.net/npm/perfops-rom”> </script>
   <!--自定义看板娘-->
   <!-- <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
   <script src="/live2d-widget/autoload.js"></script>
   <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css"/> -->

</body>

</html>